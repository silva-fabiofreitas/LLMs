{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T21:26:01.604885700Z",
     "start_time": "2024-06-27T21:26:01.586926Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa6936cd4b96a72d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T21:26:02.012451400Z",
     "start_time": "2024-06-27T21:26:02.012451400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, MarkdownListOutputParser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7bfced",
   "metadata": {},
   "source": [
    "## Especialista em direito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0908953",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "Você é um especialista jurídico e professor de direito no Brasil\n",
    "Auxilia os alunos na criação de mapa mental (MindMap)\n",
    "O mapa mental deve conter a ideia principal, ramos e subramos, sempre apontar os artigos \n",
    "Context:{input}\n",
    "Formate a saida do mapa mental em mardown \n",
    "\"\"\"\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9dbd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=TEMPLATE)\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "622a4650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Direitos Fundamentais na Constituição Brasileira\n",
      "\n",
      "- **Ideia Principal:** Direitos Fundamentais na Constituição Brasileira\n",
      "  \n",
      "- **Ramos:**\n",
      "  - **Direitos Individuais:**\n",
      "    - Artigos: 5º ao 17º\n",
      "  - **Direitos Coletivos:**\n",
      "    - Artigos: 5º ao 8º\n",
      "  - **Direitos Sociais:**\n",
      "    - Artigos: 6º ao 11º\n",
      "  - **Direitos de Nacionalidade:**\n",
      "    - Artigos: 12º e 13º\n",
      "  - **Direitos Políticos:**\n",
      "    - Artigos: 14º ao 17º\n"
     ]
    }
   ],
   "source": [
    "res = chain.invoke({\"input\": \"liste os diretiros fundamentais na constituição brasileira\"})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa77e13",
   "metadata": {},
   "source": [
    "## LLM memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f1d9f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# first initialize the large language model\n",
    "llm = OpenAI(\n",
    "\ttemperature=0.1,\n",
    ")\n",
    "\n",
    "# now initialize the conversation chain\n",
    "conversation = ConversationChain(llm=llm) # A memoria é armazenada e acumulada em uma chave chamada history\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86451174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "#Template predefinido\n",
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "200fefa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Qual o meu nome',\n",
       " 'history': 'Human: Eu sou fabio\\nAI:  Olá Fabio! É um prazer conhecê-lo. Eu sou uma inteligência artificial criada para conversar com pessoas e aprender com elas. Como posso ajudá-lo hoje?',\n",
       " 'response': ' Seu nome é Fabio. Eu sei disso porque você acabou de me dizer. Além disso, eu também tenho acesso a informações sobre você, como seu nome, idade, localização e interesses, que me ajudam a me comunicar melhor com você.'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke('Qual o meu nome')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a05233",
   "metadata": {},
   "source": [
    "Usando **ConversationBufferMemory** , usamos muitos tokens muito rapidamente e até excedemos o limite da janela de contexto até mesmo dos LLMs mais avançados disponíveis hoje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b1acd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Good morning AI!',\n",
       " 'history': '',\n",
       " 'response': \" Good morning! It's a beautiful day today. The temperature is 72 degrees Fahrenheit and the sky is mostly clear with a few scattered clouds. How are you doing today?\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "conversation_buf = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation_buf(\"Good morning AI!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba888d5",
   "metadata": {},
   "source": [
    "Ao usar **ConversationSummaryMemory** , precisamos passar um LLM para o objeto porque o resumo é alimentado por um LLM. Podemos ver o prompt usado para fazer isso aqui:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed49d9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "\tllm=llm,\n",
    "\tmemory=ConversationSummaryMemory(llm=llm) # Necessário passar um llm, pois o resumo utiliza-o para sintetizar as conversas \n",
    ")\n",
    "\n",
    "print(conversation.memory.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cccd4cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Em portugues',\n",
       " 'history': '',\n",
       " 'response': ' Olá! Eu sou um assistente de inteligência artificial e posso falar português. Como posso ajudá-lo?'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation('Em portugues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cc4bca",
   "metadata": {},
   "source": [
    "O **ConversationBufferWindowMemory** age da mesma forma que nossa “memória buffer” anterior , mas adiciona uma janela à memória. O que significa que mantemos apenas um determinado número de interações passadas antes de “esquecê- las”. Nós o usamos assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46fcbd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "\tllm=llm,\n",
    "\tmemory=ConversationBufferWindowMemory(k=1) # k=1 — isso significa que a janela se lembrará da interação mais recente\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2517ae58",
   "metadata": {},
   "source": [
    "O **ConversationSummaryBufferMemory** é uma mistura do ConversationSummaryMemory e do ConversationBufferWindowMemory . Ele resume as interações mais antigas em uma conversa, mantendo os max_token_limit tokens mais recentes em sua conversa. Ele é inicializado assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46905de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "conversation_sum_bufw = ConversationChain(\n",
    "    llm=llm, memory=ConversationSummaryBufferMemory(\n",
    "        llm=llm,\n",
    "        max_token_limit=650\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d61f403",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5262570",
   "metadata": {},
   "source": [
    "<img src='./assets/imgs/memoryBuffer.bmp' ></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d661df",
   "metadata": {},
   "source": [
    "### Memoria\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7fe4e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage \n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "llm = OpenAI(temperature=0.1)\n",
    "llm = ChatOpenAI(temperature=0.1,)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        \n",
    "     ('system', \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
    "     MessagesPlaceholder(variable_name='chat_history'),\n",
    "     ('human', \"{input}\"),\n",
    "     ('ai',':')]\n",
    ")\n",
    "\n",
    "chat_history = [\n",
    "\n",
    "]\n",
    "\n",
    "def chat(llm, input, chat_history):\n",
    "    response = llm.invoke({\n",
    "        'input':input,\n",
    "        'chat_history':chat_history\n",
    "        }\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "bb6b92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "08e63ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá, Fábio! Como posso te ajudar hoje?\n",
      "[['human', 'Eu sou fabio'], ['ai', 'Olá, Fábio! Como posso te ajudar hoje?']]\n"
     ]
    }
   ],
   "source": [
    "user_input = 'Eu sou fabio'\n",
    "# user_input = 'Qual o meu nome'\n",
    "response = chat(chain, user_input, chat_history)\n",
    "# chat_history.append(HumanMessage(content=user_input))\n",
    "# chat_history.append(AIMessage(content=response))    \n",
    "chat_history.append(['human',user_input])    \n",
    "chat_history.append(['ai',response])    \n",
    "print(response)\n",
    "print(chat_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6da31543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desculpe, não tenho essa informação. Como posso te ajudar hoje?\n",
      "[['human', 'Eu sou fabio'], ['ai', 'Olá, Fábio! Como posso te ajudar hoje?'], ['human', 'Qual o meu nome'], ['ai', 'Desculpe, não tenho essa informação. Como posso te ajudar hoje?']]\n"
     ]
    }
   ],
   "source": [
    "user_input = 'Qual o meu nome'\n",
    "response = chat(chain, user_input, chat_history)\n",
    "# chat_history.append(HumanMessage(content=user_input))\n",
    "# chat_history.append(AIMessage(content=response)) \n",
    "chat_history.append(['human',user_input])    \n",
    "chat_history.append(['ai',response])  \n",
    "# chat_history.append(f'{user_input} - {response}\\n') \n",
    "print(response)\n",
    "print(chat_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c9f6d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olá! Como posso te ajudar hoje?'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cae6af",
   "metadata": {},
   "source": [
    "### Using ConversationBufferMemory from Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9241e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chat = OpenAI(temperature=0)\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "Current conversation:\n",
    "{history} \n",
    "Human: {input}\n",
    "AI Assistant:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "# prompt = prompt.invoke({'input':'','history':'Human: Qual o meu nome\\nAI:  Seu nome é desconhecido para mim, pois sou um programa de computador e não tenho acesso a informações pessoais. Mas posso lhe chamar de \"amigo\" se preferir.\\nHuman: Eu sou fabio\\nAI:  Prazer em conhecê-lo, Fabio! Eu sou um assistente de inteligência artificial projetado para conversar e ajudar com informações. Como posso ser útil para você hoje?'})\n",
    "\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(size=5)  # sliding window buffer of size 5\n",
    "\n",
    "buffer_chain = ConversationChain(\n",
    "    prompt=prompt\n",
    "    ,llm=chat, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "65db960f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Got unsupported message type: Meu nome é fabio",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbuffer_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqual o meu nome\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projetos/LLMs/env/lib/python3.12/site-packages/langchain/chains/base.py:135\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m include_run_info \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_run_info\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    133\u001b[0m return_only_outputs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_only_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 135\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m CallbackManager\u001b[38;5;241m.\u001b[39mconfigure(\n\u001b[1;32m    137\u001b[0m     callbacks,\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[1;32m    144\u001b[0m )\n\u001b[1;32m    145\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projetos/LLMs/env/lib/python3.12/site-packages/langchain/chains/base.py:512\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    510\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mlist\u001b[39m(_input_keys)[\u001b[38;5;241m0\u001b[39m]: inputs}\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m     external_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_memory_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexternal_context)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/projetos/LLMs/env/lib/python3.12/site-packages/langchain/memory/buffer.py:66\u001b[0m, in \u001b[0;36mConversationBufferMemory.load_memory_variables\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_memory_variables\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return history buffer.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_key: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer\u001b[49m}\n",
      "File \u001b[0;32m~/projetos/LLMs/env/lib/python3.12/site-packages/langchain/memory/buffer.py:20\u001b[0m, in \u001b[0;36mConversationBufferMemory.buffer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuffer\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"String buffer of memory.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_as_messages \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_messages \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_as_str\u001b[49m\n",
      "File \u001b[0;32m~/projetos/LLMs/env/lib/python3.12/site-packages/langchain/memory/buffer.py:40\u001b[0m, in \u001b[0;36mConversationBufferMemory.buffer_as_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuffer_as_str\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Exposes the buffer as a string in case return_messages is True.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_as_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projetos/LLMs/env/lib/python3.12/site-packages/langchain/memory/buffer.py:31\u001b[0m, in \u001b[0;36mConversationBufferMemory._buffer_as_str\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_as_str\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages: List[BaseMessage]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_buffer_string\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhuman_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhuman_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mai_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mai_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projetos/LLMs/env/lib/python3.12/site-packages/langchain_core/messages/utils.py:88\u001b[0m, in \u001b[0;36mget_buffer_string\u001b[0;34m(messages, human_prefix, ai_prefix)\u001b[0m\n\u001b[1;32m     86\u001b[0m     role \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mrole\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported message type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrole\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, AIMessage) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39madditional_kwargs:\n",
      "\u001b[0;31mValueError\u001b[0m: Got unsupported message type: Meu nome é fabio"
     ]
    }
   ],
   "source": [
    "buffer_chain.invoke(input='qual o meu nome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f6ec504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\nMeu nome é Antonella Meu nome é Fabio\\nHuman: \\nAI Assistant:'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(input='',history='Meu nome é Antonella')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24644474",
   "metadata": {},
   "source": [
    "### Automatic history management\n",
    "https://python.langchain.com/v0.2/docs/tutorials/chatbot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e96198f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "store = {}\n",
    "chat_history = ChatMessageHistory()\n",
    "chat_history.add_messages(['human','Ola, sou Fabio', 'human', 'minha blusa é azul'])\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = chat_history\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "13eb23be",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "9ed10588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Você disse que se chama Fabio. Como posso ajudar você hoje?'"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Qual o meu nome?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647233a9",
   "metadata": {},
   "source": [
    "## Router chain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803b6c8",
   "metadata": {},
   "source": [
    "\n",
    "[Router](https://medium.com/@gil.fernandes/langchains-router-chains-and-callbacks-722524c4aa42)\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/expression_language/how_to/routing/\n",
    "\n",
    "Allow to dynamically select a pre-defined chain from a set of chains for a given input. In this blog we are going to explore\n",
    "\n",
    "<img src='./assets/imgs/routerChain.bmp' width=900 ></img>\n",
    "\n",
    "The following actions are executed here:\n",
    "\n",
    "The user produces a text based input.  \n",
    "The input is written to a file via a callback.  \n",
    "The router selects the most appropriate chain from five options:  \n",
    "- Python programmer\n",
    "- Poet\n",
    "- Wikipedia Expert\n",
    "- Graphical artist\n",
    "- UK, US Legal Expert   \n",
    "\n",
    "The large language model responds.  \n",
    "The output is again written to a file via a callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bca011b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anthropic'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`.\n",
    "\n",
    "Do not respond with more than one word.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"how do I call Anthropic?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bc4ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chain = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in langchain. \\\n",
    "Always answer questions starting with \"As Harrison Chase told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | ChatOpenAI()\n",
    "\n",
    "anthropic_chain = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in anthropic. \\\n",
    "Always answer questions starting with \"As Dario Amodei told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | ChatOpenAI()\n",
    "\n",
    "general_chain = PromptTemplate.from_template(\n",
    "    \"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ba0d2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As Dario Amodei told me, to use Anthropic effectively, one must first understand the principles of anthropic reasoning and apply them to the specific context or problem at hand. This involves considering the observer's perspective, taking into account various possibilities and their probabilities, and making informed judgments based on these considerations. It is important to approach the use of anthropic reasoning with a critical and analytical mindset, ensuring that all relevant factors are taken into consideration before drawing conclusions.\", response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 47, 'total_tokens': 141}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c2d8acb5-04ad-45ab-acbd-4a54ae025c3f-0', usage_metadata={'input_tokens': 47, 'output_tokens': 94, 'total_tokens': 141})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def route(info):\n",
    "    if \"anthropic\" in info[\"topic\"].lower():\n",
    "        return anthropic_chain\n",
    "    elif \"langchain\" in info[\"topic\"].lower():\n",
    "        return langchain_chain\n",
    "    else:\n",
    "        return general_chain\n",
    "    \n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(route)\n",
    "\n",
    "full_chain.invoke({\"question\": \"how do I use Anthropic?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8984ddc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As Harrison Chase told me, you can use LangChain by creating an account on the platform, uploading your content in the desired language, and selecting the language you want it translated into. The platform will then connect you with a qualified translator to complete the translation process efficiently and accurately.', response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 44, 'total_tokens': 100}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-255b3322-cb28-42d4-9460-4e6ab4f9bd09-0', usage_metadata={'input_tokens': 44, 'output_tokens': 56, 'total_tokens': 100})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"how do I use LangChain?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
