{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumário\n",
    "**1** <a href='#1-rag-in-pdf-file'> RAG PDF files</a>  \n",
    "1.2 <a href='#passando-o-resultado-da-consulta-ao-banco-vetorial-manualmente'> Passando-o-resultado-da-consulta-ao-banco-vetorial-manualmente</a>  \n",
    "1.3 <a href='#adicionar-memória-lce'>adicionar-memória-lce</a>  \n",
    "**2** <a href='#contruindo-um-assistente-de-documentação'> Assistente de documentação</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instância do LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 RAG (Retrieval Augmented Generation) in PDF file\n",
    "\n",
    "**RAG index**  \n",
    "\n",
    "<img src='assets/imgs/rag_indexing.png' width=800></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar PDF \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = 'data/pdfs/Tese_Ficha_catalografica[v3].pdf' \n",
    "loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao carregar o PDF é necessário subdividir o arquivo em pequenas seções (chunk), pois, caso o documento seja muito grande pode ultrapassar a janela de contexto do LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar os chunks com tamanho padrão\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os documentos no banco vetorial\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(), persist_directory='./db', collection_name='Tese')\n",
    "\n",
    "# Retrieve \n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval** (recuperação)\n",
    "\n",
    "<img src='assets/imgs/rag_retrieval_generation.png' width=600></img> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 72, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='regiões mais vulnerávei s, todavia os cartogramas apresentados nesta seção fornece m \\numa visão holística do problema , de modo a permitir um direcionamento pontual das \\npolíticas públicas  de acordo com as necessidades de cada município.'),\n",
       " Document(metadata={'page': 76, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='75 \\n \\nnegativo para a maioria dos municípios fluminenses. Essa proporção foi semelhante \\naos achados de Raiher e Lima (2014) na região Sul do Brasil , com 83 % e 73% dos \\nmunicípios no círculo vicio so, respectivamente a média brasileira e da Região Sul; aos \\nresultados de Oliveira , Lima e Raiher,  (2017) , na Região Nordeste , 84% dos \\nmunicípios  no círculo vicioso quando comparado com a média do Brasil, todavia, 24% \\nquando comparado  com a média da própri a região ; e Oliveira , Lima  e Barrinha,  (2019) , \\nno Estado da Bahia , 83% dos municípios do círculo da pobreza, média do Brasil, e \\n18% em relação à média do próprio estado.  Uma vez iniciado o processo de \\nsubdesenvolvimento ou de desenvolvimento humano , a chan ce de regressão é menor \\ndado o processo de causação circular cumulativa (RAIHER; LIMA, 2014 ; OLIVEIRA ; \\nLIMA; RAIHER , 2017 ; OLIVEIRA;  LIMA ; BARRINHA,  2019).  \\nAo analisar os  indicadores socioeconômicos dos municípios fluminenses'),\n",
       " Document(metadata={'page': 76, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='LIMA; RAIHER , 2017 ; OLIVEIRA;  LIMA ; BARRINHA,  2019).  \\nAo analisar os  indicadores socioeconômicos dos municípios fluminenses \\nconstatou -se que as variáveis mortalidade infantil (Mi), taxa de analfabetismo adulto \\n(T18), % de vulneráveis à pobreza (VP) e % de pobres (P) foram relacionadas \\npositivamente entre si. Essas variáveis ainda foram correlacionadas negativamente \\ncom % de 25 anos ou mais com superior comp leto (SC25), % de 18 anos ou mais com \\nfundamental completo (FC18), renda per capita ( RP), esperança de vida ao nascer \\n(EV) e probabilidade de sobrevivência até 60 anos (PS60).  As variáveis EV (32,63%), \\nPS60 (22,52%), RP (0,12%), FC18 (3,87%), SC25 (9,05%)  e EAE (64,62%) reduzem \\na chance, α=5%, de um município passar para o CVic a cada unidade incrementada; \\npor outro lado, Mi  e VP aumentam es sas chances em 23,99% e 3,63% , \\nrespectivamente.  \\nIsso demonstra que os municípios que apresentaram altas taxas de mort alidade'),\n",
       " Document(metadata={'page': 63, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='62 \\n \\nO município de Macaé, parte superior,  foi o único classificado como tendendo \\nao crescimento e tem uma alta relação com o QL, que mede a concentração da \\nindústria . Nota-se que Porto Real também está associa do ao quociente locacional, \\napesar de se encontrar na categoria  círculo vicioso . A diferença entre esses dois \\nmunicípios  refere -se aos indicadores essenciais do desenvolvim ento humano : quanto \\nmais abaixo da média, caso de Porto Real, mais próximo do círculo vicioso . Esses \\ndois casos ilustram bem a ideia de que não só a concentração industrial leva ao círculo \\nvirtuoso6. \\nAs localidades situadas na parte inferior têm caracterís ticas inversas àqueles \\nclassificados como tendendo ao crescimento, portanto apresentam baixo QL. Os \\nmunicípios classificados como “outros” não apresentaram uma relação visível com \\nnenhuma variável do estudo.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load database\n",
    "vectorstore = Chroma(persist_directory='./db', collection_name='Tese', embedding_function=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "# \n",
    "retriever.invoke('Qual o município mais vulnerável') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "rag_chain = {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='O município que precisa de atenção urgente é Porto Real, pois apresenta baixos indicadores de desenvolvimento humano e está classificado na categoria de círculo vicioso. Essa classificação indica uma relação negativa com o quociente locacional, o que sugere dificuldades no crescimento econômico.', response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 973, 'total_tokens': 1025, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-687013fb-1990-4b30-98a8-a4d76e95bb73-0', usage_metadata={'input_tokens': 973, 'output_tokens': 52, 'total_tokens': 1025})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke('Qual o município que precisa de atenção urgente?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passando o resultado da consulta ao banco vetorial manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'page': 72, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='regiões mais vulnerávei s, todavia os cartogramas apresentados nesta seção fornece m \\numa visão holística do problema , de modo a permitir um direcionamento pontual das \\npolíticas públicas  de acordo com as necessidades de cada município.'), Document(metadata={'page': 63, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='62 \\n \\nO município de Macaé, parte superior,  foi o único classificado como tendendo \\nao crescimento e tem uma alta relação com o QL, que mede a concentração da \\nindústria . Nota-se que Porto Real também está associa do ao quociente locacional, \\napesar de se encontrar na categoria  círculo vicioso . A diferença entre esses dois \\nmunicípios  refere -se aos indicadores essenciais do desenvolvim ento humano : quanto \\nmais abaixo da média, caso de Porto Real, mais próximo do círculo vicioso . Esses \\ndois casos ilustram bem a ideia de que não só a concentração industrial leva ao círculo \\nvirtuoso6. \\nAs localidades situadas na parte inferior têm caracterís ticas inversas àqueles \\nclassificados como tendendo ao crescimento, portanto apresentam baixo QL. Os \\nmunicípios classificados como “outros” não apresentaram uma relação visível com \\nnenhuma variável do estudo.'), Document(metadata={'page': 7, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='Palavras -chave : Índice de desenvolvimento humano . Crescimento econômico . \\nSaúde . Educação . Municípios do Rio de Janeiro .'), Document(metadata={'page': 76, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='75 \\n \\nnegativo para a maioria dos municípios fluminenses. Essa proporção foi semelhante \\naos achados de Raiher e Lima (2014) na região Sul do Brasil , com 83 % e 73% dos \\nmunicípios no círculo vicio so, respectivamente a média brasileira e da Região Sul; aos \\nresultados de Oliveira , Lima e Raiher,  (2017) , na Região Nordeste , 84% dos \\nmunicípios  no círculo vicioso quando comparado com a média do Brasil, todavia, 24% \\nquando comparado  com a média da própri a região ; e Oliveira , Lima  e Barrinha,  (2019) , \\nno Estado da Bahia , 83% dos municípios do círculo da pobreza, média do Brasil, e \\n18% em relação à média do próprio estado.  Uma vez iniciado o processo de \\nsubdesenvolvimento ou de desenvolvimento humano , a chan ce de regressão é menor \\ndado o processo de causação circular cumulativa (RAIHER; LIMA, 2014 ; OLIVEIRA ; \\nLIMA; RAIHER , 2017 ; OLIVEIRA;  LIMA ; BARRINHA,  2019).  \\nAo analisar os  indicadores socioeconômicos dos municípios fluminenses')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'O município que precisa de atenção urgente é Porto Real, que está classificado na categoria de \"círculo vicioso\" e apresenta indicadores essenciais de desenvolvimento humano abaixo da média. Isso indica uma situação crítica em comparação com Macaé, que tende ao crescimento.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(context=lambda input: format_docs(input[\"context\"]))\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "question = \"Qual o municipio que precisa de atenção urgente?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "# Run\n",
    "chain.invoke({\"context\": docs, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionar memória LCE\n",
    "https://github.com/TirendazAcademy/LangChain-Tutorials\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/  \n",
    "\n",
    "<img src='assets/imgs/conversational_retrieval_chain.png' width=900></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contextualizando a questão**\n",
    "\n",
    "Esta cadeia adiciona uma reformulação da consulta de entrada ao nosso mecanismo de busca, de modo que a recuperação incorpore o contexto da conversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ela é conhecida como distribuição gaussiana.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo - retorna pergunta reformulada que será passada para o retriever\n",
    "contextualize_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\":[\n",
    "            HumanMessage(content=\"Me liste os os vazamentos\"),\n",
    "            AIMessage(content=\"É uma distribuição estatística que segue uma distribuição normal\"),\n",
    "        ],\n",
    "        \"question\": \"Ela também é conhecida como outro nome?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QA chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "    \n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context = contextualized_question | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt \n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sim, a curva normal também é conhecida como distribuição gaussiana.', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 688, 'total_tokens': 702}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f33667828e', 'finish_reason': 'stop', 'logprobs': None}, id='run-0324f2f3-33be-486d-a183-fb35b312fb18-0', usage_metadata={'input_tokens': 688, 'output_tokens': 14, 'total_tokens': 702})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference\n",
    "\n",
    "rag_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\":[\n",
    "            HumanMessage(content=\"O que é uma curva normal?\"),\n",
    "            AIMessage(content=\"É uma distribuição estatística que segue uma distribuição normal\"),\n",
    "        ],\n",
    "        \"question\": \"Ela também é conhecida como outro nome?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerenciar memória \n",
    "\n",
    "Aqui, vimos como adicionar lógica de aplicativo para incorporar saídas históricas, mas ainda estamos atualizando manualmente o histórico de bate-papo e inserindo-o em cada entrada. Em um aplicativo real de perguntas e respostas, queremos alguma maneira de persistir o histórico de bate-papo e alguma maneira de inseri-lo e atualizá-lo automaticamente.\n",
    "\n",
    "Para isso, podemos usar:\n",
    "\n",
    "* [BaseChatMessageHistory](https://python.langchain.com/v0.2/api_reference/langchain/index.html#module-langchain.memory): Armazene o histórico de bate-papo.\n",
    "* [RunnableWithMessageHistory](https://python.langchain.com/v0.2/docs/how_to/message_history/): wrapper para uma cadeia LCEL e um que lida com a injeção de histórico de chat em entradas e atualizando-o após cada invocação.BaseChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"\n",
    "    Existe varias interfaces para conectar com banco de dados no langchain:\n",
    "        * langchain-postgres\n",
    "        * langchain-redis\n",
    "        * SQLChatMessageHistory (SQLite)\n",
    "\n",
    "    Args:\n",
    "        session_id (str): id para identifcar a sessão\n",
    "\n",
    "    Returns:\n",
    "        BaseChatMessageHistory: Classe responsável por armazenar o historico\n",
    "    \"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A distribuição mencionada refere-se ao histograma apresentado na Figura 16, onde os municípios classificados como \"Outros\" tendem aos extremos da distribuição. As variáveis QL, PDA, PDBA e PDCL apresentam uma sobreposição completa das faixas de variação, indicando que não há uma discriminação clara entre as categorias CVic e \"Outros\".', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 652, 'total_tokens': 728, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-016adf70-9ee5-47ce-a689-ec6805e4a4eb-0', usage_metadata={'input_tokens': 652, 'output_tokens': 76, 'total_tokens': 728})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"question\": \"Qual a distribuição que eu citei\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aboradagem usando funções auxiliares \n",
    "* create_history_aware_retriever e \n",
    "* create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='O que é uma curva normal?'),\n",
       "  AIMessage(content='É uma distribuição estatística que segue uma distribuição normal')],\n",
       " 'input': 'Ela também é conhecida como outro nome?',\n",
       " 'context': [Document(metadata={'page': 52, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='51 \\n \\nFigura 6. Matriz do d iagrama de dispersão das variáveis de estudo  \\n \\nFonte: Autor.'),\n",
       "  Document(metadata={'page': 68, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='Fonte: Autor  \\n \\nNo histograma apresentado na Figura 1 6 nota-se que os munícipios \\nclassificados como “Outros ” tendem aos estremos da dis tribuição  e que as variáveis \\nQL, PDA, PDBA e PDCL apresentam praticamente uma sobreposição completa das \\nfaixas de variação. Des sa maneira, não há uma discriminação das categorias CVic e \\n“Outros” para essas variáveis.'),\n",
       "  Document(metadata={'page': 37, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='projeção das obse rvações nos componentes principais.  \\nAs componentes são geradas pelo método SVD sobre os dados da tabela X. \\nComo X=𝑷∆𝑸𝑻 a I x L matriz dos factor scores , denotada por F é obtida:    \\n𝑭=𝑿𝑸 (6) \\nA matri z Q fornece os coeficientes (autovetores) das var iáveis para a \\ncombinação linear, também conhecida como matriz de carregamento loading matrix , \\nou matr iz de projeção, pois a multiplicação de X por Q resulta na projeção das \\nobservações nos componentes principais. De acordo com Dunteman (1989) , as \\ncomponent es também podem ser expressas algebricamente por:  \\n𝐹𝑝=∑𝑞𝑝𝑗𝑥𝑗𝐽\\n𝑗=1 (7) \\n As variáveis diferentes das observações não são representadas por suas \\nprojeções, mas, sim, por suas correlações. São p lotadas sobre um círculo de \\ncorrelações usando como coordenadas os loadings . Segundo Dunteman (19 89), \\nquan do os elementos dos vetores latentes (peso das variáveis) são transformados em'),\n",
       "  Document(metadata={'page': 49, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='maior nas faixas mais baixas (lado esquerdo do histograma) . Já os atributos PIB, RP, \\nSC25 e T18 apresentam uma assimetria à direita, isto é, uma concentração  de \\nmunicípios  nas classes mais baixas  e uma variação maior nas categorias mais altas.         \\nAs variáveis que se aproximaram de um cenário idea l (mantendo os mesmos \\nintervalos)  foram a s do grupo habitação , que possuem uma assimetria \\nnegativa/esquerda . Isso  significa que a maioria dos municíp ios fluminenses  (62) \\napresentam altas taxas  (≥90%)  de PDA, PDBA e PDCL . As demais variáveis, exceto \\nT18, ou foram simétricas ou se comportaram o inverso d a distribuição  ideal.')],\n",
       " 'answer': 'Sim, a curva normal também é conhecida como curva de Gauss.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# 2. Incorporate the retriever into a question-answering chain.\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "rag_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\":[\n",
    "            HumanMessage(content=\"O que é uma curva normal?\"),\n",
    "            AIMessage(content=\"É uma distribuição estatística que segue uma distribuição normal\"),\n",
    "        ],\n",
    "        \"input\": \"Ela também é conhecida como outro nome?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contruindo um assistente de documentação \n",
    "\n",
    "Obter documentação (bash):\n",
    "* wget -r -A.html -P langchain-docs https://api.python.langchain.com/en/latest/langchain_api_reference.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exist_colecttion(collection_name, persist_directory='db'):\n",
    "    try: \n",
    "        Chroma(\n",
    "            persist_directory='db', \n",
    "            collection_name=collection_name, \n",
    "            create_collection_if_not_exists=False\n",
    "        )\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def ingest_docs():\n",
    "    \"\"\"Carregar documentação\n",
    "    Carrega a documentação do langchain no banco vetorial\n",
    "    \"\"\"\n",
    "    loader = ReadTheDocsLoader('data/langchain-docs/api.python.langchain.com/en/latest')\n",
    "    \n",
    "    raw_docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=50)\n",
    "    \n",
    "    print(f' raw documents: {len(raw_docs)}')\n",
    "    \n",
    "    documents = text_splitter.split_documents(raw_docs[0:500])\n",
    "    \n",
    "    print(f'docs {len(documents)}')\n",
    "    \n",
    "    for doc in documents:\n",
    "        new_url = doc.metadata['source']\n",
    "        new_url = new_url.replace(\"langchain-docs\", \"https:/\")\n",
    "        doc.metadata.update({'source': new_url})  \n",
    "        \n",
    "    collection_name='docs_langchain'\n",
    "    \n",
    "    if exist_colecttion(collection_name): \n",
    "        return\n",
    "         \n",
    "    Chroma.from_documents(\n",
    "        documents=documents, \n",
    "        embedding=embeddings, \n",
    "        persist_directory='db', \n",
    "        collection_name=collection_name\n",
    "    )\n",
    "    \n",
    "    print(f'Dados persistido no banco')\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " raw documents: 4142\n",
      "docs 9378\n",
      "cole docs_langchain\n",
      "Dados persistido no banco\n"
     ]
    }
   ],
   "source": [
    "ingest_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval Augmentation Generation (RAG)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AIMessage(content='A LangChain Chain is a sequence of actions that is hardcoded to perform specific tasks. Unlike Agents, which use a language model to determine actions dynamically, Chains follow a predefined order of execution. Examples include the PALChain, which implements Program-Aided Language Models, and other specialized chains within the LangChain framework.', response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 502, 'total_tokens': 565}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-2e2dc611-f212-4176-b5d0-7a450f8b93d3-0', usage_metadata={'input_tokens': 502, 'output_tokens': 63, 'total_tokens': 565}),\n",
       " [Document(metadata={'source': 'data/https://api.python.langchain.com/en/latest/exceptions/langchain_core.exceptions.LangChainException.html'}, page_content='langchain_core.exceptions.LangChainException¶\\nclass langchain_core.exceptions.LangChainException[source]¶\\nGeneral LangChain exception.'),\n",
       "  Document(metadata={'source': 'data/https://api.python.langchain.com/en/latest/langchain_api_reference.html'}, page_content='langchain 0.2.13¶\\nlangchain.agents¶\\nAgent is a class that uses an LLM to choose a sequence of actions to take.\\nIn Chains, a sequence of actions is hardcoded. In Agents,\\na language model is used as a reasoning engine to determine which actions\\nto take and in which order.\\nAgents select and use Tools and Toolkits for actions.\\nClass hierarchy:\\nBaseSingleActionAgent --> LLMSingleActionAgent\\n                          OpenAIFunctionsAgent\\n                          XMLAgent\\n                          Agent --> <name>Agent  # Examples: ZeroShotAgent, ChatAgent'),\n",
       "  Document(metadata={'source': 'data/https://api.python.langchain.com/en/latest/experimental_api_reference.html'}, page_content='This chain is vulnerable to [arbitrary code execution](https://github.com/langchain-ai/langchain/issues/5872).\\nClasses¶\\npal_chain.base.PALChain\\nChain that implements Program-Aided Language Models (PAL).\\npal_chain.base.PALValidation([...])\\nValidation for PAL generated code.\\nlangchain_experimental.plan_and_execute¶\\nPlan-and-execute agents are planning tasks with a language model (LLM) and\\nexecuting them with a separate agent.\\nClasses¶\\nplan_and_execute.agent_executor.PlanAndExecute\\nPlan and execute a chain of steps.\\nplan_and_execute.executors.base.BaseExecutor\\nBase executor.'),\n",
       "  Document(metadata={'source': 'data/https://api.python.langchain.com/en/latest/smart_llm/langchain_experimental.smart_llm.base.SmartLLMChain.html'}, page_content='langchain_experimental.smart_llm.base.SmartLLMChain¶\\nNote\\nSmartLLMChain implements the standard Runnable Interface. 🏃\\nThe Runnable Interface has additional methods that are available on runnables, such as with_types, with_retry, assign, bind, get_graph, and more.\\nclass langchain_experimental.smart_llm.base.SmartLLMChain[source]¶\\nBases: Chain\\nChain for applying self-critique using the SmartGPT workflow.\\nSee details at https://youtu.be/wVzuvf9D9BU\\nA SmartLLMChain is an LLMChain that instead of simply passing the prompt to the LLM\\nperforms these 3 steps:')])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "collection_name='docs_langchain'\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def run_llm(query:str):\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    docsearch = Chroma(\n",
    "            persist_directory='db', \n",
    "            collection_name=collection_name, \n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "    \n",
    "    docs = docsearch.similarity_search(query)\n",
    "    \n",
    "    chat = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    \n",
    "    RAG_TEMPLATE = \"\"\"\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Answer the following question:\n",
    "\n",
    "    {question}\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "    \n",
    "    \n",
    "    rag_chain = (\n",
    "        RunnablePassthrough.assign(context=lambda input: format_docs(input[\"context\"]))\n",
    "        | prompt\n",
    "        | chat\n",
    "    )\n",
    "    \n",
    "    res = rag_chain.invoke({'question':query, 'context': docs}) \n",
    "    \n",
    "    return  res, docs\n",
    "\n",
    "\n",
    "    \n",
    "query = 'Whats is a langchain Chain?'\n",
    "    \n",
    "run_llm(query)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usando funções auxiliares: create_retrieval_chain, create_stuff_documents_chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "def run_llm_(query: str):\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    docsearch = Chroma(\n",
    "            persist_directory='db/', \n",
    "            collection_name=collection_name, \n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "    \n",
    "    chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, verbose=True)\n",
    "    \n",
    "    RAG_TEMPLATE = \"\"\"\n",
    "        You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "\n",
    "        Answer the following question:\n",
    "\n",
    "         {question}\"\"\"\n",
    "\n",
    "    retrieval_qa_chat_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "    stuff_documents_chain = create_stuff_documents_chain(chat, retrieval_qa_chat_prompt)\n",
    "\n",
    "    qa = create_retrieval_chain(\n",
    "        retriever=docsearch.as_retriever(), combine_docs_chain=stuff_documents_chain\n",
    "    )\n",
    "    result = qa.invoke(input={\"input\": query})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPENSEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "os = OpenSearchVectorSearch(\n",
    "        embedding_function=OpenAIEmbeddings(),\n",
    "        opensearch_url='localhost:9200',\n",
    "        http_auth=('admin','admin'),\n",
    "        index_name=\"tese\",\n",
    "        is_appx_search=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf', 'page': 72}, page_content='regiões mais vulnerávei s, todavia os cartogramas apresentados nesta seção fornece m \\numa visão holística do problema , de modo a permitir um direcionamento pontual das \\npolíticas públicas  de acordo com as necessidades de cada município.'),\n",
       " Document(metadata={'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf', 'page': 72}, page_content='regiões mais vulnerávei s, todavia os cartogramas apresentados nesta seção fornece m \\numa visão holística do problema , de modo a permitir um direcionamento pontual das \\npolíticas públicas  de acordo com as necessidades de cada município.'),\n",
       " Document(metadata={'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf', 'page': 72}, page_content='normalizadas (escala de 0 a 1), antes e depois da combinação, para a criação do \\nindicador. As faixas foram definidas de acordo com a escala do IDH estabelecidas \\npelo PNUD.  Dessa maneira foi possível observar os municípios mais vulneráveis ao \\ncírculo  vicioso da pobreza em relação a todas variáveis  signific ativas .  \\nDos 92 municípios , 70 (76%)  estão na categoria de muito baixo DH ; 11 (12%)  \\nna classe baixo DH ; 8 (9%) na faixa intermedi ária; 2 (2%) com alto DH ; e 1 com \\naltíssimo DH. Tomando como base a classificação anterior dos municípios (seção 4.2)  \\nas classe s extremas tendem a convergir ; a categoria círculo vicioso representa 71 % \\ncontra 76% para baixíssimo DH; e  a classe virtuoso 3%  contra 2%  alto e 1% altíssimo \\nDH. Ambos os mapas auxiliam no processo de tomada de decisão ao apontar as \\nregiões mais vulnerávei s, todavia os cartogramas apresentados nesta seção fornece m \\numa visão holística do problema , de modo a permitir um direcionamento pontual das'),\n",
       " Document(metadata={'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf', 'page': 72}, page_content='normalizadas (escala de 0 a 1), antes e depois da combinação, para a criação do \\nindicador. As faixas foram definidas de acordo com a escala do IDH estabelecidas \\npelo PNUD.  Dessa maneira foi possível observar os municípios mais vulneráveis ao \\ncírculo  vicioso da pobreza em relação a todas variáveis  signific ativas .  \\nDos 92 municípios , 70 (76%)  estão na categoria de muito baixo DH ; 11 (12%)  \\nna classe baixo DH ; 8 (9%) na faixa intermedi ária; 2 (2%) com alto DH ; e 1 com \\naltíssimo DH. Tomando como base a classificação anterior dos municípios (seção 4.2)  \\nas classe s extremas tendem a convergir ; a categoria círculo vicioso representa 71 % \\ncontra 76% para baixíssimo DH; e  a classe virtuoso 3%  contra 2%  alto e 1% altíssimo \\nDH. Ambos os mapas auxiliam no processo de tomada de decisão ao apontar as \\nregiões mais vulnerávei s, todavia os cartogramas apresentados nesta seção fornece m \\numa visão holística do problema , de modo a permitir um direcionamento pontual das')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtros = {\n",
    "    \"bool\": {\n",
    "        \"filter\": \n",
    "            {\"match\": {\"metadata.page\": 72}},\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "retriever = os.as_retriever(search_kwargs={'filter':filtros})\n",
    "# \n",
    "retriever.invoke('Qual o município mais vulnerável') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf', 'page': 72}, page_content='regiões mais vulnerávei s, todavia os cartogramas apresentados nesta seção fornece m \\numa visão holística do problema , de modo a permitir um direcionamento pontual das \\npolíticas públicas  de acordo com as necessidades de cada município.'),\n",
       " Document(metadata={'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf', 'page': 72}, page_content='regiões mais vulnerávei s, todavia os cartogramas apresentados nesta seção fornece m \\numa visão holística do problema , de modo a permitir um direcionamento pontual das \\npolíticas públicas  de acordo com as necessidades de cada município.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter = {\"bool\": {\"filter\": {\"term\": {\"metadata.page\": 72}}}}\n",
    "docs = os.similarity_search(\n",
    "    \"vulnerabilidade\",\n",
    "    # search_type=\"painless_scripting\",\n",
    "    # space_type=\"cosineSimilarity\",\n",
    "    # pre_filter=filter,\n",
    "    # efficient_filter=filter\n",
    "    boolean_filter=filter,\n",
    "    # metadata_field='metadata'\n",
    ")\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
