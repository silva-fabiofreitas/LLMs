{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum√°rio\n",
    "**1** <a href='#1-rag-in-pdf-file'> RAG PDF files</a>  \n",
    "1.2 <a href='#passando-o-resultado-da-consulta-ao-banco-vetorial-manualmente'> Passando-o-resultado-da-consulta-ao-banco-vetorial-manualmente</a>  \n",
    "1.3 <a href='#adicionar-mem√≥ria-lce'>adicionar-mem√≥ria-lce</a>  \n",
    "**2** <a href='#contruindo-um-assistente-de-documenta√ß√£o'> Assistente de documenta√ß√£o</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inst√¢ncia do LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 RAG (Retrieval Augmented Generation) in PDF file\n",
    "\n",
    "**RAG index**  \n",
    "\n",
    "<img src='assets/imgs/rag_indexing.png' width=800></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar PDF \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = 'data/pdfs/Tese_Ficha_catalografica[v3].pdf' \n",
    "loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao carregar o PDF √© necess√°rio subdividir o arquivo em pequenas se√ß√µes (chunk), pois, caso o documento seja muito grande pode ultrapassar a janela de contexto do LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar os chunks com tamanho padr√£o\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os documentos no banco vetorial\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(), persist_directory='./db', collection_name='Tese')\n",
    "\n",
    "# Retrieve \n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval** (recupera√ß√£o)\n",
    "\n",
    "<img src='assets/imgs/rag_retrieval_generation.png' width=600></img> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 72, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='regi√µes mais vulner√°vei s, todavia os cartogramas apresentados nesta se√ß√£o fornece m \\numa vis√£o hol√≠stica do problema , de modo a permitir um direcionamento pontual das \\npol√≠ticas p√∫blicas  de acordo com as necessidades de cada munic√≠pio.'),\n",
       " Document(metadata={'page': 76, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='75 \\n \\nnegativo para a maioria dos munic√≠pios fluminenses. Essa propor√ß√£o foi semelhante \\naos achados de Raiher e Lima (2014) na regi√£o Sul do Brasil , com 83 % e 73% dos \\nmunic√≠pios no c√≠rculo vicio so, respectivamente a m√©dia brasileira e da Regi√£o Sul; aos \\nresultados de Oliveira , Lima e Raiher,  (2017) , na Regi√£o Nordeste , 84% dos \\nmunic√≠pios  no c√≠rculo vicioso quando comparado com a m√©dia do Brasil, todavia, 24% \\nquando comparado  com a m√©dia da pr√≥pri a regi√£o ; e Oliveira , Lima  e Barrinha,  (2019) , \\nno Estado da Bahia , 83% dos munic√≠pios do c√≠rculo da pobreza, m√©dia do Brasil, e \\n18% em rela√ß√£o √† m√©dia do pr√≥prio estado.  Uma vez iniciado o processo de \\nsubdesenvolvimento ou de desenvolvimento humano , a chan ce de regress√£o √© menor \\ndado o processo de causa√ß√£o circular cumulativa (RAIHER; LIMA, 2014 ; OLIVEIRA ; \\nLIMA; RAIHER , 2017 ; OLIVEIRA;  LIMA ; BARRINHA,  2019).  \\nAo analisar os  indicadores socioecon√¥micos dos munic√≠pios fluminenses'),\n",
       " Document(metadata={'page': 76, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='LIMA; RAIHER , 2017 ; OLIVEIRA;  LIMA ; BARRINHA,  2019).  \\nAo analisar os  indicadores socioecon√¥micos dos munic√≠pios fluminenses \\nconstatou -se que as vari√°veis mortalidade infantil (Mi), taxa de analfabetismo adulto \\n(T18), % de vulner√°veis √† pobreza (VP) e % de pobres (P) foram relacionadas \\npositivamente entre si. Essas vari√°veis ainda foram correlacionadas negativamente \\ncom % de 25 anos ou mais com superior comp leto (SC25), % de 18 anos ou mais com \\nfundamental completo (FC18), renda per capita ( RP), esperan√ßa de vida ao nascer \\n(EV) e probabilidade de sobreviv√™ncia at√© 60 anos (PS60).  As vari√°veis EV (32,63%), \\nPS60 (22,52%), RP (0,12%), FC18 (3,87%), SC25 (9,05%)  e EAE (64,62%) reduzem \\na chance, Œ±=5%, de um munic√≠pio passar para o CVic a cada unidade incrementada; \\npor outro lado, Mi  e VP aumentam es sas chances em 23,99% e 3,63% , \\nrespectivamente.  \\nIsso demonstra que os munic√≠pios que apresentaram altas taxas de mort alidade'),\n",
       " Document(metadata={'page': 63, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='62 \\n \\nO munic√≠pio de Maca√©, parte superior,  foi o √∫nico classificado como tendendo \\nao crescimento e tem uma alta rela√ß√£o com o QL, que mede a concentra√ß√£o da \\nind√∫stria . Nota-se que Porto Real tamb√©m est√° associa do ao quociente locacional, \\napesar de se encontrar na categoria  c√≠rculo vicioso . A diferen√ßa entre esses dois \\nmunic√≠pios  refere -se aos indicadores essenciais do desenvolvim ento humano : quanto \\nmais abaixo da m√©dia, caso de Porto Real, mais pr√≥ximo do c√≠rculo vicioso . Esses \\ndois casos ilustram bem a ideia de que n√£o s√≥ a concentra√ß√£o industrial leva ao c√≠rculo \\nvirtuoso6. \\nAs localidades situadas na parte inferior t√™m caracter√≠s ticas inversas √†queles \\nclassificados como tendendo ao crescimento, portanto apresentam baixo QL. Os \\nmunic√≠pios classificados como ‚Äúoutros‚Äù n√£o apresentaram uma rela√ß√£o vis√≠vel com \\nnenhuma vari√°vel do estudo.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load database\n",
    "vectorstore = Chroma(persist_directory='./db', collection_name='Tese', embedding_function=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "# \n",
    "retriever.invoke('Qual o munic√≠pio mais vulner√°vel') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "rag_chain = {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='O munic√≠pio que precisa de aten√ß√£o urgente √© Porto Real, pois apresenta baixos indicadores de desenvolvimento humano e est√° classificado na categoria de c√≠rculo vicioso. Essa classifica√ß√£o indica uma rela√ß√£o negativa com o quociente locacional, o que sugere dificuldades no crescimento econ√¥mico.', response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 973, 'total_tokens': 1025, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-687013fb-1990-4b30-98a8-a4d76e95bb73-0', usage_metadata={'input_tokens': 973, 'output_tokens': 52, 'total_tokens': 1025})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke('Qual o munic√≠pio que precisa de aten√ß√£o urgente?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passando o resultado da consulta ao banco vetorial manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'page': 72, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='regi√µes mais vulner√°vei s, todavia os cartogramas apresentados nesta se√ß√£o fornece m \\numa vis√£o hol√≠stica do problema , de modo a permitir um direcionamento pontual das \\npol√≠ticas p√∫blicas  de acordo com as necessidades de cada munic√≠pio.'), Document(metadata={'page': 63, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='62 \\n \\nO munic√≠pio de Maca√©, parte superior,  foi o √∫nico classificado como tendendo \\nao crescimento e tem uma alta rela√ß√£o com o QL, que mede a concentra√ß√£o da \\nind√∫stria . Nota-se que Porto Real tamb√©m est√° associa do ao quociente locacional, \\napesar de se encontrar na categoria  c√≠rculo vicioso . A diferen√ßa entre esses dois \\nmunic√≠pios  refere -se aos indicadores essenciais do desenvolvim ento humano : quanto \\nmais abaixo da m√©dia, caso de Porto Real, mais pr√≥ximo do c√≠rculo vicioso . Esses \\ndois casos ilustram bem a ideia de que n√£o s√≥ a concentra√ß√£o industrial leva ao c√≠rculo \\nvirtuoso6. \\nAs localidades situadas na parte inferior t√™m caracter√≠s ticas inversas √†queles \\nclassificados como tendendo ao crescimento, portanto apresentam baixo QL. Os \\nmunic√≠pios classificados como ‚Äúoutros‚Äù n√£o apresentaram uma rela√ß√£o vis√≠vel com \\nnenhuma vari√°vel do estudo.'), Document(metadata={'page': 7, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='Palavras -chave : √çndice de desenvolvimento humano . Crescimento econ√¥mico . \\nSa√∫de . Educa√ß√£o . Munic√≠pios do Rio de Janeiro .'), Document(metadata={'page': 76, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='75 \\n \\nnegativo para a maioria dos munic√≠pios fluminenses. Essa propor√ß√£o foi semelhante \\naos achados de Raiher e Lima (2014) na regi√£o Sul do Brasil , com 83 % e 73% dos \\nmunic√≠pios no c√≠rculo vicio so, respectivamente a m√©dia brasileira e da Regi√£o Sul; aos \\nresultados de Oliveira , Lima e Raiher,  (2017) , na Regi√£o Nordeste , 84% dos \\nmunic√≠pios  no c√≠rculo vicioso quando comparado com a m√©dia do Brasil, todavia, 24% \\nquando comparado  com a m√©dia da pr√≥pri a regi√£o ; e Oliveira , Lima  e Barrinha,  (2019) , \\nno Estado da Bahia , 83% dos munic√≠pios do c√≠rculo da pobreza, m√©dia do Brasil, e \\n18% em rela√ß√£o √† m√©dia do pr√≥prio estado.  Uma vez iniciado o processo de \\nsubdesenvolvimento ou de desenvolvimento humano , a chan ce de regress√£o √© menor \\ndado o processo de causa√ß√£o circular cumulativa (RAIHER; LIMA, 2014 ; OLIVEIRA ; \\nLIMA; RAIHER , 2017 ; OLIVEIRA;  LIMA ; BARRINHA,  2019).  \\nAo analisar os  indicadores socioecon√¥micos dos munic√≠pios fluminenses')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'O munic√≠pio que precisa de aten√ß√£o urgente √© Porto Real, que est√° classificado na categoria de \"c√≠rculo vicioso\" e apresenta indicadores essenciais de desenvolvimento humano abaixo da m√©dia. Isso indica uma situa√ß√£o cr√≠tica em compara√ß√£o com Maca√©, que tende ao crescimento.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(context=lambda input: format_docs(input[\"context\"]))\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "question = \"Qual o municipio que precisa de aten√ß√£o urgente?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "# Run\n",
    "chain.invoke({\"context\": docs, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionar mem√≥ria LCE\n",
    "https://github.com/TirendazAcademy/LangChain-Tutorials\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/  \n",
    "\n",
    "<img src='assets/imgs/conversational_retrieval_chain.png' width=900></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contextualizando a quest√£o**\n",
    "\n",
    "Esta cadeia adiciona uma reformula√ß√£o da consulta de entrada ao nosso mecanismo de busca, de modo que a recupera√ß√£o incorpore o contexto da conversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ela √© conhecida como distribui√ß√£o gaussiana.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo - retorna pergunta reformulada que ser√° passada para o retriever\n",
    "contextualize_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\":[\n",
    "            HumanMessage(content=\"Me liste os os vazamentos\"),\n",
    "            AIMessage(content=\"√â uma distribui√ß√£o estat√≠stica que segue uma distribui√ß√£o normal\"),\n",
    "        ],\n",
    "        \"question\": \"Ela tamb√©m √© conhecida como outro nome?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QA chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "    \n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context = contextualized_question | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt \n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sim, a curva normal tamb√©m √© conhecida como distribui√ß√£o gaussiana.', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 688, 'total_tokens': 702}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f33667828e', 'finish_reason': 'stop', 'logprobs': None}, id='run-0324f2f3-33be-486d-a183-fb35b312fb18-0', usage_metadata={'input_tokens': 688, 'output_tokens': 14, 'total_tokens': 702})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference\n",
    "\n",
    "rag_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\":[\n",
    "            HumanMessage(content=\"O que √© uma curva normal?\"),\n",
    "            AIMessage(content=\"√â uma distribui√ß√£o estat√≠stica que segue uma distribui√ß√£o normal\"),\n",
    "        ],\n",
    "        \"question\": \"Ela tamb√©m √© conhecida como outro nome?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerenciar mem√≥ria \n",
    "\n",
    "Aqui, vimos como adicionar l√≥gica de aplicativo para incorporar sa√≠das hist√≥ricas, mas ainda estamos atualizando manualmente o hist√≥rico de bate-papo e inserindo-o em cada entrada. Em um aplicativo real de perguntas e respostas, queremos alguma maneira de persistir o hist√≥rico de bate-papo e alguma maneira de inseri-lo e atualiz√°-lo automaticamente.\n",
    "\n",
    "Para isso, podemos usar:\n",
    "\n",
    "* [BaseChatMessageHistory](https://python.langchain.com/v0.2/api_reference/langchain/index.html#module-langchain.memory): Armazene o hist√≥rico de bate-papo.\n",
    "* [RunnableWithMessageHistory](https://python.langchain.com/v0.2/docs/how_to/message_history/): wrapper para uma cadeia LCEL e um que lida com a inje√ß√£o de hist√≥rico de chat em entradas e atualizando-o ap√≥s cada invoca√ß√£o.BaseChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"\n",
    "    Existe varias interfaces para conectar com banco de dados no langchain:\n",
    "        * langchain-postgres\n",
    "        * langchain-redis\n",
    "        * SQLChatMessageHistory (SQLite)\n",
    "\n",
    "    Args:\n",
    "        session_id (str): id para identifcar a sess√£o\n",
    "\n",
    "    Returns:\n",
    "        BaseChatMessageHistory: Classe respons√°vel por armazenar o historico\n",
    "    \"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A distribui√ß√£o mencionada refere-se ao histograma apresentado na Figura 16, onde os munic√≠pios classificados como \"Outros\" tendem aos extremos da distribui√ß√£o. As vari√°veis QL, PDA, PDBA e PDCL apresentam uma sobreposi√ß√£o completa das faixas de varia√ß√£o, indicando que n√£o h√° uma discrimina√ß√£o clara entre as categorias CVic e \"Outros\".', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 652, 'total_tokens': 728, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-016adf70-9ee5-47ce-a689-ec6805e4a4eb-0', usage_metadata={'input_tokens': 652, 'output_tokens': 76, 'total_tokens': 728})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"question\": \"Qual a distribui√ß√£o que eu citei\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aboradagem usando fun√ß√µes auxiliares \n",
    "* create_history_aware_retriever e \n",
    "* create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='O que √© uma curva normal?'),\n",
       "  AIMessage(content='√â uma distribui√ß√£o estat√≠stica que segue uma distribui√ß√£o normal')],\n",
       " 'input': 'Ela tamb√©m √© conhecida como outro nome?',\n",
       " 'context': [Document(metadata={'page': 52, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='51 \\n \\nFigura 6. Matriz do d iagrama de dispers√£o das vari√°veis de estudo  \\n \\nFonte: Autor.'),\n",
       "  Document(metadata={'page': 68, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='Fonte: Autor  \\n \\nNo histograma apresentado na Figura 1 6 nota-se que os mun√≠cipios \\nclassificados como ‚ÄúOutros ‚Äù tendem aos estremos da dis tribui√ß√£o  e que as vari√°veis \\nQL, PDA, PDBA e PDCL apresentam praticamente uma sobreposi√ß√£o completa das \\nfaixas de varia√ß√£o. Des sa maneira, n√£o h√° uma discrimina√ß√£o das categorias CVic e \\n‚ÄúOutros‚Äù para essas vari√°veis.'),\n",
       "  Document(metadata={'page': 37, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='proje√ß√£o das obse rva√ß√µes nos componentes principais.  \\nAs componentes s√£o geradas pelo m√©todo SVD sobre os dados da tabela X. \\nComo X=ùë∑‚àÜùë∏ùëª a I x L matriz dos factor scores , denotada por F √© obtida:    \\nùë≠=ùëøùë∏ (6) \\nA matri z Q fornece os coeficientes (autovetores) das var i√°veis para a \\ncombina√ß√£o linear, tamb√©m conhecida como matriz de carregamento loading matrix , \\nou matr iz de proje√ß√£o, pois a multiplica√ß√£o de X por Q resulta na proje√ß√£o das \\nobserva√ß√µes nos componentes principais. De acordo com Dunteman (1989) , as \\ncomponent es tamb√©m podem ser expressas algebricamente por:  \\nùêπùëù=‚àëùëûùëùùëóùë•ùëóùêΩ\\nùëó=1 (7) \\n As vari√°veis diferentes das observa√ß√µes n√£o s√£o representadas por suas \\nproje√ß√µes, mas, sim, por suas correla√ß√µes. S√£o p lotadas sobre um c√≠rculo de \\ncorrela√ß√µes usando como coordenadas os loadings . Segundo Dunteman (19 89), \\nquan do os elementos dos vetores latentes (peso das vari√°veis) s√£o transformados em'),\n",
       "  Document(metadata={'page': 49, 'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf'}, page_content='maior nas faixas mais baixas (lado esquerdo do histograma) . J√° os atributos PIB, RP, \\nSC25 e T18 apresentam uma assimetria √† direita, isto √©, uma concentra√ß√£o  de \\nmunic√≠pios  nas classes mais baixas  e uma varia√ß√£o maior nas categorias mais altas.         \\nAs vari√°veis que se aproximaram de um cen√°rio idea l (mantendo os mesmos \\nintervalos)  foram a s do grupo habita√ß√£o , que possuem uma assimetria \\nnegativa/esquerda . Isso  significa que a maioria dos munic√≠p ios fluminenses  (62) \\napresentam altas taxas  (‚â•90%)  de PDA, PDBA e PDCL . As demais vari√°veis, exceto \\nT18, ou foram sim√©tricas ou se comportaram o inverso d a distribui√ß√£o  ideal.')],\n",
       " 'answer': 'Sim, a curva normal tamb√©m √© conhecida como curva de Gauss.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# 2. Incorporate the retriever into a question-answering chain.\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "rag_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\":[\n",
    "            HumanMessage(content=\"O que √© uma curva normal?\"),\n",
    "            AIMessage(content=\"√â uma distribui√ß√£o estat√≠stica que segue uma distribui√ß√£o normal\"),\n",
    "        ],\n",
    "        \"input\": \"Ela tamb√©m √© conhecida como outro nome?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contruindo um assistente de documenta√ß√£o \n",
    "\n",
    "Obter documenta√ß√£o (bash):\n",
    "* wget -r -A.html -P langchain-docs https://api.python.langchain.com/en/latest/langchain_api_reference.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exist_colecttion(collection_name, persist_directory='db'):\n",
    "    try: \n",
    "        Chroma(\n",
    "            persist_directory='db', \n",
    "            collection_name=collection_name, \n",
    "            create_collection_if_not_exists=False\n",
    "        )\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def ingest_docs():\n",
    "    \"\"\"Carregar documenta√ß√£o\n",
    "    Carrega a documenta√ß√£o do langchain no banco vetorial\n",
    "    \"\"\"\n",
    "    loader = ReadTheDocsLoader('data/langchain-docs/api.python.langchain.com/en/latest')\n",
    "    \n",
    "    raw_docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=50)\n",
    "    \n",
    "    print(f' raw documents: {len(raw_docs)}')\n",
    "    \n",
    "    documents = text_splitter.split_documents(raw_docs[0:500])\n",
    "    \n",
    "    print(f'docs {len(documents)}')\n",
    "    \n",
    "    for doc in documents:\n",
    "        new_url = doc.metadata['source']\n",
    "        new_url = new_url.replace(\"langchain-docs\", \"https:/\")\n",
    "        doc.metadata.update({'source': new_url})  \n",
    "        \n",
    "    collection_name='docs_langchain'\n",
    "    \n",
    "    if exist_colecttion(collection_name): \n",
    "        return\n",
    "         \n",
    "    Chroma.from_documents(\n",
    "        documents=documents, \n",
    "        embedding=embeddings, \n",
    "        persist_directory='db', \n",
    "        collection_name=collection_name\n",
    "    )\n",
    "    \n",
    "    print(f'Dados persistido no banco')\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " raw documents: 4142\n",
      "docs 9378\n",
      "cole docs_langchain\n",
      "Dados persistido no banco\n"
     ]
    }
   ],
   "source": [
    "ingest_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval Augmentation Generation (RAG)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(AIMessage(content='A LangChain Chain is a sequence of actions that is hardcoded to perform specific tasks. Unlike Agents, which use a language model to determine actions dynamically, Chains follow a predefined order of execution. Examples include the PALChain, which implements Program-Aided Language Models, and other specialized chains within the LangChain framework.', response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 502, 'total_tokens': 565}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-2e2dc611-f212-4176-b5d0-7a450f8b93d3-0', usage_metadata={'input_tokens': 502, 'output_tokens': 63, 'total_tokens': 565}),\n",
       " [Document(metadata={'source': 'data/https://api.python.langchain.com/en/latest/exceptions/langchain_core.exceptions.LangChainException.html'}, page_content='langchain_core.exceptions.LangChainException¬∂\\nclass langchain_core.exceptions.LangChainException[source]¬∂\\nGeneral LangChain exception.'),\n",
       "  Document(metadata={'source': 'data/https://api.python.langchain.com/en/latest/langchain_api_reference.html'}, page_content='langchain 0.2.13¬∂\\nlangchain.agents¬∂\\nAgent is a class that uses an LLM to choose a sequence of actions to take.\\nIn Chains, a sequence of actions is hardcoded. In Agents,\\na language model is used as a reasoning engine to determine which actions\\nto take and in which order.\\nAgents select and use Tools and Toolkits for actions.\\nClass hierarchy:\\nBaseSingleActionAgent --> LLMSingleActionAgent\\n                          OpenAIFunctionsAgent\\n                          XMLAgent\\n                          Agent --> <name>Agent  # Examples: ZeroShotAgent, ChatAgent'),\n",
       "  Document(metadata={'source': 'data/https://api.python.langchain.com/en/latest/experimental_api_reference.html'}, page_content='This chain is vulnerable to [arbitrary code execution](https://github.com/langchain-ai/langchain/issues/5872).\\nClasses¬∂\\npal_chain.base.PALChain\\nChain that implements Program-Aided Language Models (PAL).\\npal_chain.base.PALValidation([...])\\nValidation for PAL generated code.\\nlangchain_experimental.plan_and_execute¬∂\\nPlan-and-execute agents are planning tasks with a language model (LLM) and\\nexecuting them with a separate agent.\\nClasses¬∂\\nplan_and_execute.agent_executor.PlanAndExecute\\nPlan and execute a chain of steps.\\nplan_and_execute.executors.base.BaseExecutor\\nBase executor.'),\n",
       "  Document(metadata={'source': 'data/https://api.python.langchain.com/en/latest/smart_llm/langchain_experimental.smart_llm.base.SmartLLMChain.html'}, page_content='langchain_experimental.smart_llm.base.SmartLLMChain¬∂\\nNote\\nSmartLLMChain implements the standard Runnable Interface. üèÉ\\nThe Runnable Interface has additional methods that are available on runnables, such as with_types, with_retry, assign, bind, get_graph, and more.\\nclass langchain_experimental.smart_llm.base.SmartLLMChain[source]¬∂\\nBases: Chain\\nChain for applying self-critique using the SmartGPT workflow.\\nSee details at https://youtu.be/wVzuvf9D9BU\\nA SmartLLMChain is an LLMChain that instead of simply passing the prompt to the LLM\\nperforms these 3 steps:')])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "collection_name='docs_langchain'\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def run_llm(query:str):\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    docsearch = Chroma(\n",
    "            persist_directory='db', \n",
    "            collection_name=collection_name, \n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "    \n",
    "    docs = docsearch.similarity_search(query)\n",
    "    \n",
    "    chat = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    \n",
    "    RAG_TEMPLATE = \"\"\"\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Answer the following question:\n",
    "\n",
    "    {question}\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "    \n",
    "    \n",
    "    rag_chain = (\n",
    "        RunnablePassthrough.assign(context=lambda input: format_docs(input[\"context\"]))\n",
    "        | prompt\n",
    "        | chat\n",
    "    )\n",
    "    \n",
    "    res = rag_chain.invoke({'question':query, 'context': docs}) \n",
    "    \n",
    "    return  res, docs\n",
    "\n",
    "\n",
    "    \n",
    "query = 'Whats is a langchain Chain?'\n",
    "    \n",
    "run_llm(query)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usando fun√ß√µes auxiliares: create_retrieval_chain, create_stuff_documents_chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "def run_llm_(query: str):\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    docsearch = Chroma(\n",
    "            persist_directory='db/', \n",
    "            collection_name=collection_name, \n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "    \n",
    "    chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, verbose=True)\n",
    "    \n",
    "    RAG_TEMPLATE = \"\"\"\n",
    "        You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "\n",
    "        Answer the following question:\n",
    "\n",
    "         {question}\"\"\"\n",
    "\n",
    "    retrieval_qa_chat_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "    stuff_documents_chain = create_stuff_documents_chain(chat, retrieval_qa_chat_prompt)\n",
    "\n",
    "    qa = create_retrieval_chain(\n",
    "        retriever=docsearch.as_retriever(), combine_docs_chain=stuff_documents_chain\n",
    "    )\n",
    "    result = qa.invoke(input={\"input\": query})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPENSEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "os = OpenSearchVectorSearch(\n",
    "        embedding_function=OpenAIEmbeddings(),\n",
    "        opensearch_url='localhost:9200',\n",
    "        http_auth=('admin','admin'),\n",
    "        index_name=\"tese\",\n",
    "        is_appx_search=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf', 'page': 72}, page_content='regi√µes mais vulner√°vei s, todavia os cartogramas apresentados nesta se√ß√£o fornece m \\numa vis√£o hol√≠stica do problema , de modo a permitir um direcionamento pontual das \\npol√≠ticas p√∫blicas  de acordo com as necessidades de cada munic√≠pio.'),\n",
       " Document(metadata={'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf', 'page': 72}, page_content='regi√µes mais vulner√°vei s, todavia os cartogramas apresentados nesta se√ß√£o fornece m \\numa vis√£o hol√≠stica do problema , de modo a permitir um direcionamento pontual das \\npol√≠ticas p√∫blicas  de acordo com as necessidades de cada munic√≠pio.'),\n",
       " Document(metadata={'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf', 'page': 72}, page_content='normalizadas (escala de 0 a 1), antes e depois da combina√ß√£o, para a cria√ß√£o do \\nindicador. As faixas foram definidas de acordo com a escala do IDH estabelecidas \\npelo PNUD.  Dessa maneira foi poss√≠vel observar os munic√≠pios mais vulner√°veis ao \\nc√≠rculo  vicioso da pobreza em rela√ß√£o a todas vari√°veis  signific ativas .  \\nDos 92 munic√≠pios , 70 (76%)  est√£o na categoria de muito baixo DH ; 11 (12%)  \\nna classe baixo DH ; 8 (9%) na faixa intermedi √°ria; 2 (2%) com alto DH ; e 1 com \\nalt√≠ssimo DH. Tomando como base a classifica√ß√£o anterior dos munic√≠pios (se√ß√£o 4.2)  \\nas classe s extremas tendem a convergir ; a categoria c√≠rculo vicioso representa 71 % \\ncontra 76% para baix√≠ssimo DH; e  a classe virtuoso 3%  contra 2%  alto e 1% alt√≠ssimo \\nDH. Ambos os mapas auxiliam no processo de tomada de decis√£o ao apontar as \\nregi√µes mais vulner√°vei s, todavia os cartogramas apresentados nesta se√ß√£o fornece m \\numa vis√£o hol√≠stica do problema , de modo a permitir um direcionamento pontual das'),\n",
       " Document(metadata={'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf', 'page': 72}, page_content='normalizadas (escala de 0 a 1), antes e depois da combina√ß√£o, para a cria√ß√£o do \\nindicador. As faixas foram definidas de acordo com a escala do IDH estabelecidas \\npelo PNUD.  Dessa maneira foi poss√≠vel observar os munic√≠pios mais vulner√°veis ao \\nc√≠rculo  vicioso da pobreza em rela√ß√£o a todas vari√°veis  signific ativas .  \\nDos 92 munic√≠pios , 70 (76%)  est√£o na categoria de muito baixo DH ; 11 (12%)  \\nna classe baixo DH ; 8 (9%) na faixa intermedi √°ria; 2 (2%) com alto DH ; e 1 com \\nalt√≠ssimo DH. Tomando como base a classifica√ß√£o anterior dos munic√≠pios (se√ß√£o 4.2)  \\nas classe s extremas tendem a convergir ; a categoria c√≠rculo vicioso representa 71 % \\ncontra 76% para baix√≠ssimo DH; e  a classe virtuoso 3%  contra 2%  alto e 1% alt√≠ssimo \\nDH. Ambos os mapas auxiliam no processo de tomada de decis√£o ao apontar as \\nregi√µes mais vulner√°vei s, todavia os cartogramas apresentados nesta se√ß√£o fornece m \\numa vis√£o hol√≠stica do problema , de modo a permitir um direcionamento pontual das')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtros = {\n",
    "    \"bool\": {\n",
    "        \"filter\": \n",
    "            {\"match\": {\"metadata.page\": 72}},\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "retriever = os.as_retriever(search_kwargs={'filter':filtros})\n",
    "# \n",
    "retriever.invoke('Qual o munic√≠pio mais vulner√°vel') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf', 'page': 72}, page_content='regi√µes mais vulner√°vei s, todavia os cartogramas apresentados nesta se√ß√£o fornece m \\numa vis√£o hol√≠stica do problema , de modo a permitir um direcionamento pontual das \\npol√≠ticas p√∫blicas  de acordo com as necessidades de cada munic√≠pio.'),\n",
       " Document(metadata={'source': 'data/pdfs/Tese_Ficha_catalografica[v3].pdf', 'page': 72}, page_content='regi√µes mais vulner√°vei s, todavia os cartogramas apresentados nesta se√ß√£o fornece m \\numa vis√£o hol√≠stica do problema , de modo a permitir um direcionamento pontual das \\npol√≠ticas p√∫blicas  de acordo com as necessidades de cada munic√≠pio.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter = {\"bool\": {\"filter\": {\"term\": {\"metadata.page\": 72}}}}\n",
    "docs = os.similarity_search(\n",
    "    \"vulnerabilidade\",\n",
    "    # search_type=\"painless_scripting\",\n",
    "    # space_type=\"cosineSimilarity\",\n",
    "    # pre_filter=filter,\n",
    "    # efficient_filter=filter\n",
    "    boolean_filter=filter,\n",
    "    # metadata_field='metadata'\n",
    ")\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
